{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjcullBo/Q0dcPbZhU+IUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naiksahana/CNN-classification/blob/main/Car_vs_Bikeipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Network"
      ],
      "metadata": {
        "id": "J50ag2enWOZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "4aXggoYyWYjn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iVdEUDhWVcPJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tf.__version__\n"
      ],
      "metadata": {
        "id": "cAZO7TwzWh1V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a8a43377-4b91-4188-fd4c-5849cba18259"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wEZ_26GPuANU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce15b1b-07da-4a6e-e5e7-5c45eb4a9fb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1 - Data Preprocessing"
      ],
      "metadata": {
        "id": "QddirXoeWisl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the Training set"
      ],
      "metadata": {
        "id": "1leD9a8hWoLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/Train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "metadata": {
        "id": "fWiy7BqrWpD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4de3bf-7291-47b7-ac89-594a516224a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1041 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the Test set"
      ],
      "metadata": {
        "id": "I6_minidWtzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/Test',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "metadata": {
        "id": "dJBPtZv8WugJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8ac23e-d622-4a70-f4f2-fb40d2864271"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 494 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 - Building the CNN"
      ],
      "metadata": {
        "id": "NkWDN9IoWxMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialising the CNN"
      ],
      "metadata": {
        "id": "hHRUcRBDW70h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn = tf.keras.models.Sequential()\n"
      ],
      "metadata": {
        "id": "xS_4Wj88Wzkf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - Convolution"
      ],
      "metadata": {
        "id": "ke9MzZmFXCdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n"
      ],
      "metadata": {
        "id": "1E_M56XBXEp_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 - Pooling"
      ],
      "metadata": {
        "id": "81505_n_XHA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "JKBlHZXOXJqJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a second convolutional layer"
      ],
      "metadata": {
        "id": "vjRWLPwUXY5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "qMmrodSvXbhq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 - Flattening"
      ],
      "metadata": {
        "id": "AuxXiGBLXdrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "rzps5JmBXf_Y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 - Full Connection"
      ],
      "metadata": {
        "id": "1a8sHUK_XiQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "metadata": {
        "id": "ywkddmUdXkKJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 - Output Layer"
      ],
      "metadata": {
        "id": "-ExvTjA1Xmk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "GDuZ6oTtXonQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3 - Training the CNN"
      ],
      "metadata": {
        "id": "USRtfN_nXrS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling the CNN"
      ],
      "metadata": {
        "id": "cS6pdDoGXtYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
      ],
      "metadata": {
        "id": "KlYNTX8fXvL7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the CNN on the Training set and evaluating it on the Test set"
      ],
      "metadata": {
        "id": "zFXgQ1T2Xxqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "metadata": {
        "id": "c8jw02V7X1_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb22a6e6-7bc5-4c74-b057-230cc80be06b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "33/33 [==============================] - 415s 13s/step - loss: 0.6980 - accuracy: 0.5466 - val_loss: 0.6414 - val_accuracy: 0.7186\n",
            "Epoch 2/25\n",
            "33/33 [==============================] - 125s 4s/step - loss: 0.5852 - accuracy: 0.7137 - val_loss: 0.6184 - val_accuracy: 0.7348\n",
            "Epoch 3/25\n",
            "33/33 [==============================] - 119s 4s/step - loss: 0.5474 - accuracy: 0.7368 - val_loss: 0.7257 - val_accuracy: 0.7348\n",
            "Epoch 4/25\n",
            "33/33 [==============================] - 110s 3s/step - loss: 0.5394 - accuracy: 0.7368 - val_loss: 0.5706 - val_accuracy: 0.7551\n",
            "Epoch 5/25\n",
            "33/33 [==============================] - 111s 3s/step - loss: 0.4860 - accuracy: 0.7752 - val_loss: 0.5786 - val_accuracy: 0.7045\n",
            "Epoch 6/25\n",
            "33/33 [==============================] - 109s 3s/step - loss: 0.4696 - accuracy: 0.7829 - val_loss: 0.5131 - val_accuracy: 0.7510\n",
            "Epoch 7/25\n",
            "33/33 [==============================] - 115s 4s/step - loss: 0.4502 - accuracy: 0.7858 - val_loss: 0.5643 - val_accuracy: 0.7389\n",
            "Epoch 8/25\n",
            "33/33 [==============================] - 108s 3s/step - loss: 0.3989 - accuracy: 0.8117 - val_loss: 0.5673 - val_accuracy: 0.7409\n",
            "Epoch 9/25\n",
            "33/33 [==============================] - 111s 3s/step - loss: 0.3922 - accuracy: 0.8146 - val_loss: 0.5926 - val_accuracy: 0.7368\n",
            "Epoch 10/25\n",
            "33/33 [==============================] - 107s 3s/step - loss: 0.3394 - accuracy: 0.8444 - val_loss: 0.6360 - val_accuracy: 0.7510\n",
            "Epoch 11/25\n",
            "33/33 [==============================] - 115s 4s/step - loss: 0.3288 - accuracy: 0.8482 - val_loss: 0.6186 - val_accuracy: 0.7591\n",
            "Epoch 12/25\n",
            "33/33 [==============================] - 121s 4s/step - loss: 0.3116 - accuracy: 0.8646 - val_loss: 0.5817 - val_accuracy: 0.7733\n",
            "Epoch 13/25\n",
            "33/33 [==============================] - 115s 3s/step - loss: 0.2894 - accuracy: 0.8722 - val_loss: 0.5727 - val_accuracy: 0.7713\n",
            "Epoch 14/25\n",
            "33/33 [==============================] - 114s 3s/step - loss: 0.2646 - accuracy: 0.8799 - val_loss: 0.5943 - val_accuracy: 0.7490\n",
            "Epoch 15/25\n",
            "33/33 [==============================] - 117s 4s/step - loss: 0.2765 - accuracy: 0.8895 - val_loss: 0.7279 - val_accuracy: 0.7490\n",
            "Epoch 16/25\n",
            "33/33 [==============================] - 115s 4s/step - loss: 0.2308 - accuracy: 0.8991 - val_loss: 0.7232 - val_accuracy: 0.7692\n",
            "Epoch 17/25\n",
            "33/33 [==============================] - 112s 3s/step - loss: 0.2273 - accuracy: 0.8972 - val_loss: 1.0044 - val_accuracy: 0.7308\n",
            "Epoch 18/25\n",
            "33/33 [==============================] - 114s 3s/step - loss: 0.2194 - accuracy: 0.9030 - val_loss: 0.6221 - val_accuracy: 0.7692\n",
            "Epoch 19/25\n",
            "33/33 [==============================] - 113s 3s/step - loss: 0.2027 - accuracy: 0.9097 - val_loss: 0.6759 - val_accuracy: 0.7551\n",
            "Epoch 20/25\n",
            "33/33 [==============================] - 123s 4s/step - loss: 0.1997 - accuracy: 0.9107 - val_loss: 0.7097 - val_accuracy: 0.7753\n",
            "Epoch 21/25\n",
            "33/33 [==============================] - 108s 3s/step - loss: 0.1655 - accuracy: 0.9260 - val_loss: 0.8494 - val_accuracy: 0.7470\n",
            "Epoch 22/25\n",
            "33/33 [==============================] - 110s 3s/step - loss: 0.1665 - accuracy: 0.9241 - val_loss: 0.6166 - val_accuracy: 0.7874\n",
            "Epoch 23/25\n",
            "33/33 [==============================] - 113s 3s/step - loss: 0.1624 - accuracy: 0.9376 - val_loss: 0.7333 - val_accuracy: 0.7935\n",
            "Epoch 24/25\n",
            "33/33 [==============================] - 108s 3s/step - loss: 0.1479 - accuracy: 0.9462 - val_loss: 0.8212 - val_accuracy: 0.7915\n",
            "Epoch 25/25\n",
            "33/33 [==============================] - 123s 4s/step - loss: 0.1090 - accuracy: 0.9577 - val_loss: 0.8854 - val_accuracy: 0.7591\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f2508156050>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4 - Making a single prediction"
      ],
      "metadata": {
        "id": "J_dJATJ_Xz06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('/content/drive/MyDrive/singleprediction/Bikeorcar1.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'Car'\n",
        "else:\n",
        "  prediction = 'Bike'"
      ],
      "metadata": {
        "id": "wR7gw4wZX8K1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6dbf95-c920-476b-ec2f-81234f4b7435"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bah31wWVdOm5",
        "outputId": "5f1ee9d2-d34a-40d3-f472-9f88cfde71d1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Car\n"
          ]
        }
      ]
    }
  ]
}